{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t6S2m9tqeVr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSICs7IlPrfh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lcyxzKY6W0yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "umjH5rmsW5Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df12= pd.read_csv(\"three_calpha_loop_tryingfor20.csv\")\n"
      ],
      "metadata": {
        "id": "NQ6b0sLlS9zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df12)"
      ],
      "metadata": {
        "id": "S-PtghFbQQTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####we have to split the csv file into smaller dataframes based on the end of each dataframe##########################\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pcLSNuc4a0e1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GHFIwEjQLbrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the step size for splitting\n",
        "step_size = 666\n",
        "\n",
        "# Calculate the total number of splits\n",
        "num_splits = (len(df12) + step_size - 1) // step_size\n",
        "\n",
        "# Create a dictionary to store the dataframes\n",
        "dfs = {}\n",
        "\n",
        "# Iterate over the splits\n",
        "for i in range(num_splits):\n",
        "    start_idx = i * step_size\n",
        "    end_idx = min((i + 1) * step_size, len(df12))\n",
        "    dfs[f'df_{start_idx}_{end_idx}'] = df12.iloc[start_idx:end_idx, :]\n",
        "\n",
        "# If you want to print the keys and the corresponding dataframes\n",
        "for key, value in dfs.items():\n",
        "    print(key)\n",
        "    print(value)\n"
      ],
      "metadata": {
        "id": "zclE4Fk_a9eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gYn6Og_cVejP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as plt"
      ],
      "metadata": {
        "id": "G926-mtLiGBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1P7397Tp1QDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install igraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0bj54xZ5b61",
        "outputId": "bc3b68be-019d-4b2d-cb4b-1cf4269246e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting igraph\n",
            "  Downloading igraph-0.11.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting texttable>=1.6.2 (from igraph)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: texttable, igraph\n",
            "Successfully installed igraph-0.11.4 texttable-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import igraph as ig\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wunibuXAd_Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the upper threshold values for filtering\n",
        "upper_threshold = 8.0\n",
        "\n",
        "# Define a dictionary to store clustering coefficient values for each dataframe\n",
        "clustering_coefficient = {}\n",
        "\n",
        "# Iterate over the dataframes in the dictionary\n",
        "for key, df in dfs.items():\n",
        "    filtered_df = df[df[\"value\"] <= upper_threshold]\n",
        "\n",
        "    # Create a graph\n",
        "    g = ig.Graph.TupleList(filtered_df[['X1', 'X2']].itertuples(index=False), directed=False)\n",
        "\n",
        "    # Calculate clustering coefficient\n",
        "    clustering = g.transitivity_local_undirected(mode=\"zero\")\n",
        "\n",
        "    # Store the clustering coefficient values in the dictionary\n",
        "    clustering_coefficient[key] = list(zip(g.vs.indices, clustering))"
      ],
      "metadata": {
        "id": "nfUNP6JsMsyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print clustering coefficient values for each dataframe\n",
        "for key, values in clustering_coefficient.items():\n",
        "    print(f\"Clustering coefficient for DataFrame {key}:\")\n",
        "    for node_index, coefficient_value in values:\n",
        "        print(f\"Node {node_index}: Clustering Coefficient {coefficient_value}\")"
      ],
      "metadata": {
        "id": "8AMtjuKEMxdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the  upper threshold values for filtering\n",
        "upper_threshold = 8.0\n",
        "\n",
        "# Define a dictionary to store betweenness centrality values for each dataframe\n",
        "betweenness_centrality = {}\n",
        "\n",
        "# Iterate over the dataframes in the dictionary\n",
        "for key, df in dfs.items():\n",
        "    filtered_df2 = df[(df[\"value\"] <= upper_threshold)]\n",
        "\n",
        "    g = ig.Graph.TupleList(filtered_df2[['X1', 'X2']].itertuples(index=False), directed=False)\n",
        "\n",
        "    # Calculate betweenness centrality\n",
        "    centrality = g.betweenness()\n",
        "\n",
        "    # Store the betweenness centrality values in the dictionary\n",
        "    betweenness_centrality[key] = list(zip(g.vs.indices, centrality))"
      ],
      "metadata": {
        "id": "WqKgVhcrMyR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print betweenness centrality values for each dataframe\n",
        "for key, values in betweenness_centrality.items():\n",
        "    print(f\"Betweenness centrality for DataFrame {key}:\")\n",
        "    for node_index, centrality_value in values:\n",
        "        print(f\"Node {node_index}: {centrality_value}\")"
      ],
      "metadata": {
        "id": "eFoh6jsXNbO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the  upper threshold values for filtering\n",
        "upper_threshold = 8.0\n",
        "\n",
        "# Define a dictionary to store degree dist values for each dataframe\n",
        "degree_dist = {}\n",
        "\n",
        "# Iterate over the dataframes in the dictionary\n",
        "for key, df in dfs.items():\n",
        "    filtered_df2 = df[(df[\"value\"] <= upper_threshold)]\n",
        "\n",
        "    g = ig.Graph.TupleList(filtered_df2[['X1', 'X2']].itertuples(index=False), directed=False)\n",
        "\n",
        "    # Calculate degree distribution\n",
        "    degrees = g.degree()\n",
        "\n",
        "    # Store the degree dist values in the dictionary\n",
        "    degree_dist[key] = list(zip(g.vs.indices, degrees))"
      ],
      "metadata": {
        "id": "WPl4k3R72snX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print degree distribution values for each dataframe\n",
        "for key, values in degree_dist.items():\n",
        "    print(f\"Degree distribution for DataFrame {key}:\")\n",
        "    for node_index, degree_value in values:\n",
        "        print(f\"Node {node_index}:  {degree_value}\")"
      ],
      "metadata": {
        "id": "QorsR4qI3iIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the  upper threshold values for filtering\n",
        "upper_threshold = 8.0\n",
        "\n",
        "# Define a dictionary to store closeness cent values for each dataframe\n",
        "closeness_centrality = {}\n",
        "\n",
        "# Iterate over the dataframes in the dictionary\n",
        "for key, df in dfs.items():\n",
        "    filtered_df2 = df[(df[\"value\"] <= upper_threshold)]\n",
        "\n",
        "    g = ig.Graph.TupleList(filtered_df2[['X1', 'X2']].itertuples(index=False), directed=False)\n",
        "\n",
        "    # Calculate closeness centrality\n",
        "    closeness = g.closeness()\n",
        "\n",
        "    # Store the closeness centrality values in the dictionary\n",
        "    closeness_centrality[key] = list(zip(g.vs.indices, closeness))"
      ],
      "metadata": {
        "id": "n2kbTDSL5Xhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print closeness cent values for each dataframe\n",
        "for key, values in closeness_centrality.items():\n",
        "    print(f\"Closeness Centrality for DataFrame {key}:\")\n",
        "    for node_index, closeness_value in values:\n",
        "        print(f\"Node {node_index}:  {closeness_value}\")"
      ],
      "metadata": {
        "id": "kwBJEaMELTYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the  upper threshold values for filtering\n",
        "upper_threshold = 8.0\n",
        "\n",
        "# Define a dictionary to store degree dist values for each dataframe\n",
        "eigen_centrality = {}\n",
        "\n",
        "# Iterate over the dataframes in the dictionary\n",
        "for key, df in dfs.items():\n",
        "    filtered_df2 = df[(df[\"value\"] <= upper_threshold)]\n",
        "\n",
        "    g = ig.Graph.TupleList(filtered_df2[['X1', 'X2']].itertuples(index=False), directed=False)\n",
        "\n",
        "    # Calculate eigenvector distribution\n",
        "    eigenness = g.eigenvector_centrality()\n",
        "\n",
        "    # Store the eigenvector centrality values in the dictionary\n",
        "    eigen_centrality[key] = list(zip(g.vs.indices, eigenness))"
      ],
      "metadata": {
        "id": "rzkeRtTqLu0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print EIGENVECTOR cent values for each dataframe\n",
        "for key, values in eigen_centrality.items():\n",
        "    print(f\"Eigenvector Centrality for DataFrame {key}:\")\n",
        "    for node_index, eigenness_value in values:\n",
        "        print(f\"Node {node_index}:  {eigenness_value}\")"
      ],
      "metadata": {
        "id": "d9sg6l1tNyyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QmoaLNOGVex-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a dictionary to store betweenness centrality values for each dataframe\n",
        "betweenness_centrality_df = {}\n",
        "\n",
        "# Iterate over the dataframes in the dictionary\n",
        "for key, values in betweenness_centrality.items():\n",
        "    # Create a DataFrame from the list of tuples\n",
        "    df = pd.DataFrame(values, columns=['Node', 'Betweenness Centrality'])\n",
        "\n",
        "    # Add the DataFrame to the dictionary\n",
        "    betweenness_centrality_df[key] = df\n",
        "\n"
      ],
      "metadata": {
        "id": "4IBPv-Tu8hLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LX3JKV9DVdUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Concatenate all dataframes in betweenness_centrality_df into a single dataframe\n",
        "combined_df = pd.concat(betweenness_centrality_df.values())\n",
        "\n",
        "# Calculate the average of \"Betweenness Centrality\" values for each node\n",
        "average_df = combined_df.groupby('Node')['Betweenness Centrality'].mean().reset_index()\n",
        "\n",
        "# Print the resulting dataframe\n",
        "print(average_df)"
      ],
      "metadata": {
        "id": "TMhFiP-wA9jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_df.to_csv('btw_fnf.csv')\n"
      ],
      "metadata": {
        "id": "1U2X3CIKQEjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a dictionary to store  degree dist values for each dataframe\n",
        "degree_df1 = {}\n",
        "\n",
        "# Iterate over the dataframes in the dictionary\n",
        "for key, values in degree_dist.items():\n",
        "    # Create a DataFrame from the list of tuples\n",
        "    df1 = pd.DataFrame(values, columns=['Node', 'Degree'])\n",
        "\n",
        "    # Add the DataFrame to the dictionary\n",
        "    degree_df1[key] = df1\n",
        "\n"
      ],
      "metadata": {
        "id": "vvzoDKP-52eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Concatenate all dataframes in degree_df into a single dataframe\n",
        "combined_df1 = pd.concat(degree_df1.values())\n",
        "\n",
        "# Calculate the average of \"DEGREE DISTRIBUTION\" values for each node\n",
        "average_df1 = combined_df1.groupby('Node')['Degree'].mean().reset_index()\n",
        "\n",
        "# Print the resulting dataframe\n",
        "print(average_df1)"
      ],
      "metadata": {
        "id": "vQHoesW5J_Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_df1.to_csv('degdist_fnf.csv')\n"
      ],
      "metadata": {
        "id": "s0i9xyTpP-O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a dictionary to store closeness centrality values for each dataframe\n",
        "closeness_centrality_df = {}\n",
        "\n",
        "# Iterate over the dataframes in the dictionary\n",
        "for key, values in closeness_centrality.items():\n",
        "    # Create a DataFrame from the list of tuples\n",
        "    df_bc = pd.DataFrame(values, columns=['Node', 'Closeness Centrality'])\n",
        "\n",
        "    # Add the DataFrame to the dictionary\n",
        "    closeness_centrality_df[key] = df_bc"
      ],
      "metadata": {
        "id": "wdrC_kyuMMdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Concatenate all dataframes in closeness_centrality_df into a single dataframe\n",
        "combined_dfcc = pd.concat(closeness_centrality_df.values())\n",
        "\n",
        "# Calculate the average of \"CLOSENESS CENTRALITY\" values for each node\n",
        "average_dfcc = combined_dfcc.groupby('Node')['Closeness Centrality'].mean().reset_index()\n",
        "\n",
        "# Print the resulting dataframe\n",
        "print(average_dfcc)"
      ],
      "metadata": {
        "id": "3QXa9GvvMk-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_dfcc.to_csv('closeness_avg_fnf.csv')\n"
      ],
      "metadata": {
        "id": "rglMBZtqPza2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a dictionary to store eigenvector centrality values for each dataframe\n",
        "eigenvector_centrality_df = {}\n",
        "\n",
        "# Iterate over the dataframes in the dictionary\n",
        "for key, values in eigen_centrality.items():\n",
        "    # Create a DataFrame from the list of tuples\n",
        "    df_ec = pd.DataFrame(values, columns=['Node', 'Eigenvector Centrality'])\n",
        "\n",
        "    # Add the DataFrame to the dictionary\n",
        "    eigenvector_centrality_df[key] = df_ec\n"
      ],
      "metadata": {
        "id": "YUbVzz5SM-4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Concatenate all dataframes in eigenvector_centrality_df into a single dataframe\n",
        "combined_dfec = pd.concat(eigenvector_centrality_df.values())\n",
        "\n",
        "# Calculate the average of \"EIGENVECTOR CENTRALITY\" values for each node\n",
        "average_dfec = combined_dfec.groupby('Node') ['Eigenvector Centrality'].mean().reset_index()\n",
        "\n",
        "# Print the resulting dataframe\n",
        "print(average_dfec)"
      ],
      "metadata": {
        "id": "Ee1itImpOPjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_dfec.to_csv('ec_avg_fnf.csv')\n"
      ],
      "metadata": {
        "id": "Wtqvau7jPq3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary to store clustering coefficient values for each dataframe\n",
        "clustering_coefficient_df = {}\n",
        "\n",
        "# Iterate over the dataframes in the dictionary\n",
        "for key, values in clustering_coefficient.items():\n",
        "    # Create a DataFrame from the list of tuples\n",
        "    df_cluster = pd.DataFrame(values, columns=['Node', 'Clustering Coefficient'])\n",
        "\n",
        "    # Add the DataFrame to the dictionary\n",
        "    clustering_coefficient_df[key] = df_cluster\n"
      ],
      "metadata": {
        "id": "Mx4Rav0ENgFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Concatenate all dataframes in clustering_coefficient_df into a single dataframe\n",
        "combined_dfcluster = pd.concat(clustering_coefficient_df.values())\n",
        "\n",
        "# Calculate the average of \"Clustering Coefficient\" values for each node\n",
        "average_df_cluster = combined_dfcluster.groupby('Node') ['Clustering Coefficient'].mean().reset_index()\n",
        "\n",
        "# Print the resulting dataframe\n",
        "print(average_df_cluster)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0ibZmZRN7sH",
        "outputId": "00230fe3-6d29-47e0-b864-477022ba54cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Node  Clustering Coefficient\n",
            "0      0                0.717778\n",
            "1      1                0.850000\n",
            "2      2                0.594048\n",
            "3      3                0.615873\n",
            "4      4                0.816667\n",
            "5      5                0.598942\n",
            "6      6                0.495152\n",
            "7      7                0.544649\n",
            "8      8                0.687169\n",
            "9      9                0.572143\n",
            "10    10                0.484499\n",
            "11    11                0.618367\n",
            "12    12                0.780000\n",
            "13    13                0.960000\n",
            "14    14                0.875556\n",
            "15    15                0.659259\n",
            "16    16                0.467417\n",
            "17    17                0.380932\n",
            "18    18                0.920000\n",
            "19    19                1.000000\n",
            "20    20                0.571561\n",
            "21    21                0.530556\n",
            "22    22                0.901111\n",
            "23    23                0.906667\n",
            "24    24                0.565476\n",
            "25    25                0.500000\n",
            "26    26                0.619127\n",
            "27    27                0.887778\n",
            "28    28                0.467174\n",
            "29    29                0.436667\n",
            "30    30                0.553519\n",
            "31    31                0.426279\n",
            "32    32                0.456902\n",
            "33    33                0.742857\n",
            "34    34                0.739286\n",
            "35    35                0.881111\n",
            "36    36                0.733333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# saving the dataframe\n",
        "average_df_cluster.to_csv('cluster_avg_fnf.csv')\n"
      ],
      "metadata": {
        "id": "PoBVn9usOhfE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}